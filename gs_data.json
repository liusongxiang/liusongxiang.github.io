{"container_type": "Author", "filled": ["basics", "publications", "indices", "counts"], "scholar_id": "4fD1l28AAAAJ", "source": "AUTHOR_PROFILE_PAGE", "name": "Songxiang Liu (刘颂湘)", "url_picture": "https://scholar.googleusercontent.com/citations?view_op=view_photo&user=4fD1l28AAAAJ&citpid=10", "affiliation": "Meituan multi-modal team, PhD (The Chinese University of Hong Kong)", "interests": ["Multi-Modal", "LLM", "Audio foundation model", "Speech synthesis"], "email_domain": "@meituan.com", "homepage": "https://liusongxiang.github.io/", "citedby": 2531, "publications": {"4fD1l28AAAAJ:TQgYirikUcIC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Uniaudio: An audio foundation model toward universal audio generation", "pub_year": "2023"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:TQgYirikUcIC", "num_citations": 268, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=6416249801268158267,7553768456221728553,18002924992754452046", "cites_id": ["6416249801268158267", "7553768456221728553", "18002924992754452046"]}, "4fD1l28AAAAJ:qUcmZB5y_30C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Hifi-codec: Group-residual vector quantization for high fidelity audio codec", "pub_year": "2023"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:qUcmZB5y_30C", "num_citations": 208, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=16848014406171770614", "cites_id": ["16848014406171770614"]}, "4fD1l28AAAAJ:blknAaTinKkC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Kimi-audio technical report", "pub_year": "2025"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:blknAaTinKkC", "num_citations": 157, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=13268094458635878451,11832037031026641577,2937086630324741387", "cites_id": ["13268094458635878451", "11832037031026641577", "2937086630324741387"]}, "4fD1l28AAAAJ:9yKSN-GCB0IC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Speech emotion recognition using capsule networks", "pub_year": "2019"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:9yKSN-GCB0IC", "num_citations": 152, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=6866674333681259224", "cites_id": ["6866674333681259224"]}, "4fD1l28AAAAJ:IWHjjKOFINEC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Instructtts: Modelling expressive tts in discrete latent space with natural language style prompt", "pub_year": "2024"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:IWHjjKOFINEC", "num_citations": 148, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=16890941306445601074", "cites_id": ["16890941306445601074"]}, "4fD1l28AAAAJ:0EnyYjriUFMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Any-to-Many Voice Conversion with Location-Relative Sequence-to-Sequence Modeling", "pub_year": "2020"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:0EnyYjriUFMC", "num_citations": 148, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=7372879383090815131", "cites_id": ["7372879383090815131"]}, "4fD1l28AAAAJ:j3f4tGmQtD8C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Spark-tts: An efficient llm-based text-to-speech model with single-stream decoupled speech tokens", "pub_year": "2025"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:j3f4tGmQtD8C", "num_citations": 106, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=1211497024443391202", "cites_id": ["1211497024443391202"]}, "4fD1l28AAAAJ:dhFuZR0502QC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "DiffGAN-TTS: High-Fidelity and Efficient Text-to-Speech with Denoising Diffusion GANs", "pub_year": "2022"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:dhFuZR0502QC", "num_citations": 93, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=14441056820237830855", "cites_id": ["14441056820237830855"]}, "4fD1l28AAAAJ:mB3voiENLucC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "The singing voice conversion challenge 2023", "pub_year": "2023"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:mB3voiENLucC", "num_citations": 91, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=16383041847465451120", "cites_id": ["16383041847465451120"]}, "4fD1l28AAAAJ:KlAtU1dfN6UC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Diffsvc: A diffusion probabilistic model for singing voice conversion", "pub_year": "2021"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:KlAtU1dfN6UC", "num_citations": 91, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=11297443352512290343", "cites_id": ["11297443352512290343"]}, "4fD1l28AAAAJ:eQOLeE2rZwMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Adversarial attacks on spoofing countermeasures of automatic speaker verification", "pub_year": "2019"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:eQOLeE2rZwMC", "num_citations": 86, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=5942007230433315201", "cites_id": ["5942007230433315201"]}, "4fD1l28AAAAJ:ufrVoPGSRksC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Defense against adversarial attacks on spoofing countermeasures of ASV", "pub_year": "2020"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:ufrVoPGSRksC", "num_citations": 80, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17274619265438276350", "cites_id": ["17274619265438276350"]}, "4fD1l28AAAAJ:2osOgNQ5qMEC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Voice Conversion Across Arbitrary Speakers Based on a Single Target-Speaker Utterance.", "pub_year": "2018"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:2osOgNQ5qMEC", "num_citations": 72, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=1838168555704570350", "cites_id": ["1838168555704570350"]}, "4fD1l28AAAAJ:8k81kl-MbHgC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Fastsvc: Fast cross-domain singing voice conversion with feature-wise linear modulation", "pub_year": "2020"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:8k81kl-MbHgC", "num_citations": 62, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=7943316403064107198", "cites_id": ["7943316403064107198"]}, "4fD1l28AAAAJ:Se3iqnhoufwC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "End-to-end voice conversion via cross-modal knowledge distillation for dysarthric speech reconstruction", "pub_year": "2020"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:Se3iqnhoufwC", "num_citations": 60, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=13337003060448968010", "cites_id": ["13337003060448968010"]}, "4fD1l28AAAAJ:_FxGoFyzp5QC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "End-to-end accent conversion without using native utterances", "pub_year": "2020"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:_FxGoFyzp5QC", "num_citations": 59, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=891216156754147457", "cites_id": ["891216156754147457"]}, "4fD1l28AAAAJ:Tyk-4Ss8FVUC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "End-to-end code-switched tts with mix of monolingual recordings", "pub_year": "2019"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:Tyk-4Ss8FVUC", "num_citations": 54, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=5057995397960606397", "cites_id": ["5057995397960606397"]}, "4fD1l28AAAAJ:isC4tDSrTZIC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Simplespeech 2: Towards simple and efficient text-to-speech with flow-based scalar latent transformer diffusion models", "pub_year": "2025"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:isC4tDSrTZIC", "num_citations": 50, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=14605884257628173023", "cites_id": ["14605884257628173023"]}, "4fD1l28AAAAJ:mVmsd5A6BfQC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Speech emotion recognition using sequential capsule networks", "pub_year": "2021"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:mVmsd5A6BfQC", "num_citations": 44, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=2717168213267941817", "cites_id": ["2717168213267941817"]}, "4fD1l28AAAAJ:3fE2CSJIrl8C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Vara-tts: Non-autoregressive text-to-speech synthesis based on very deep vae with residual attention", "pub_year": "2021"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:3fE2CSJIrl8C", "num_citations": 40, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17555399644562579779", "cites_id": ["17555399644562579779"]}, "4fD1l28AAAAJ:hqOjcs7Dif8C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Transferring source style in non-parallel voice conversion", "pub_year": "2020"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:hqOjcs7Dif8C", "num_citations": 31, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=15071601370803151941", "cites_id": ["15071601370803151941"]}, "4fD1l28AAAAJ:UebtZRa9Y70C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Code-switched speech synthesis using bilingual phonetic posteriorgram with only monolingual corpora", "pub_year": "2020"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:UebtZRa9Y70C", "num_citations": 31, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=13692853317908287002", "cites_id": ["13692853317908287002"]}, "4fD1l28AAAAJ:k_IJM867U9cC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Artificial intelligence-aided method to detect uterine fibroids in ultrasound images: a retrospective study", "pub_year": "2023"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:k_IJM867U9cC", "num_citations": 30, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=16574414579387572676", "cites_id": ["16574414579387572676"]}, "4fD1l28AAAAJ:M3NEmzRMIkIC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Application of artificial intelligence technology in the field of orthopedics: a narrative review", "pub_year": "2024"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:M3NEmzRMIkIC", "num_citations": 27, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17050748107920972958", "cites_id": ["17050748107920972958"]}, "4fD1l28AAAAJ:4DMP91E08xMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "ASR-GLUE: A new multi-task benchmark for asr-robust natural language understanding", "pub_year": "2021"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:4DMP91E08xMC", "num_citations": 23, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=12835921506512356436", "cites_id": ["12835921506512356436"]}, "4fD1l28AAAAJ:ZeXyd9-uunAC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "NoreSpeech: Knowledge distillation based conditional diffusion model for noise-robust expressive TTS", "pub_year": "2022"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:ZeXyd9-uunAC", "num_citations": 21, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=4594110725415956343", "cites_id": ["4594110725415956343"]}, "4fD1l28AAAAJ:UeHWp8X0CEIC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Rapid Style Adaptation Using Residual Error Embedding for Expressive Speech Synthesis.", "pub_year": "2018"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:UeHWp8X0CEIC", "num_citations": 21, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=9376059394877645332", "cites_id": ["9376059394877645332"]}, "4fD1l28AAAAJ:roLk4NBRz8UC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Emotional voice conversion with cycle-consistent adversarial network", "pub_year": "2020"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:roLk4NBRz8UC", "num_citations": 19, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17533728865518501807", "cites_id": ["17533728865518501807"]}, "4fD1l28AAAAJ:ldfaerwXgEUC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "MoonCast: High-Quality Zero-Shot Podcast Generation", "pub_year": "2025"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:ldfaerwXgEUC", "num_citations": 17, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=13077581285165083759", "cites_id": ["13077581285165083759"]}, "4fD1l28AAAAJ:iH-uZ7U-co4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Codec-superb@ slt 2024: A lightweight benchmark for neural audio codec models", "pub_year": "2024"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:iH-uZ7U-co4C", "num_citations": 17, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=3736893770146517120", "cites_id": ["3736893770146517120"]}, "4fD1l28AAAAJ:YFjsv_pBGBYC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Modelling expressive tts in discrete latent space with natural language style prompt", "pub_year": "2023"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:YFjsv_pBGBYC", "num_citations": 17, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=260471511044184686", "cites_id": ["260471511044184686"]}, "4fD1l28AAAAJ:NMxIlDl6LWMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Almtokenizer: A low-bitrate and semantic-rich audio codec tokenizer for audio language modeling", "pub_year": "2025"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:NMxIlDl6LWMC", "num_citations": 16, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=9681962965744793410", "cites_id": ["9681962965744793410"]}, "4fD1l28AAAAJ:YOwf2qJgpHMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Learning explicit prosody models and deep speaker embeddings for atypical voice conversion", "pub_year": "2020"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:YOwf2qJgpHMC", "num_citations": 16, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=4687766061567714906", "cites_id": ["4687766061567714906"]}, "4fD1l28AAAAJ:WF5omc3nYNoC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Multi-layer content interaction through quaternion product for visual question answering", "pub_year": "2020"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:WF5omc3nYNoC", "num_citations": 15, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=558761707762633192", "cites_id": ["558761707762633192"]}, "4fD1l28AAAAJ:W7OEmFMy1HYC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Jointly Trained Conversion Model and WaveNet Vocoder for Non-Parallel Voice Conversion Using Mel-Spectrograms and Phonetic Posteriorgrams.", "pub_year": "2019"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:W7OEmFMy1HYC", "num_citations": 15, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=7751971885196598745", "cites_id": ["7751971885196598745"]}, "4fD1l28AAAAJ:7PzlFSSx8tAC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Speaker identity preservation in dysarthric speech reconstruction by adversarial speaker adaptation", "pub_year": "2022"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:7PzlFSSx8tAC", "num_citations": 14, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=5995674066518595379", "cites_id": ["5995674066518595379"]}, "4fD1l28AAAAJ:9ZlFYXVOiuMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Meta-voice: Fast few-shot style transfer for expressive voice cloning using meta learning", "pub_year": "2021"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:9ZlFYXVOiuMC", "num_citations": 14, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=3714809576001804441", "cites_id": ["3714809576001804441"]}, "4fD1l28AAAAJ:Wp0gIr-vW9MC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Referee: Towards reference-free cross-speaker style transfer with low-quality data for expressive speech synthesis", "pub_year": "2022"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:Wp0gIr-vW9MC", "num_citations": 12, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=3548429165806688521", "cites_id": ["3548429165806688521"]}, "4fD1l28AAAAJ:MXK_kJrjxJIC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Exemplar-based emotive speech synthesis", "pub_year": "2021"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:MXK_kJrjxJIC", "num_citations": 12, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=3655307404366070235", "cites_id": ["3655307404366070235"]}, "4fD1l28AAAAJ:qjMakFHDy7sC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Feature based adaptation for speaking style synthesis", "pub_year": "2018"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:qjMakFHDy7sC", "num_citations": 12, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=9206384030054884631", "cites_id": ["9206384030054884631"]}, "4fD1l28AAAAJ:LkGwnXOMwfcC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Multi-target emotional voice conversion with neural vocoders", "pub_year": "2020"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:LkGwnXOMwfcC", "num_citations": 11, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=2988641404480545486", "cites_id": ["2988641404480545486"]}, "4fD1l28AAAAJ:K3LRdlH-MEoC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "others. 2025. Kimi-audio technical report", "pub_year": "1"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:K3LRdlH-MEoC", "num_citations": 11, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=9867961703254528307", "cites_id": ["9867961703254528307"]}, "4fD1l28AAAAJ:-f6ydRqryjwC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Diverse and expressive speech prosody prediction with denoising diffusion probabilistic model", "pub_year": "2023"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:-f6ydRqryjwC", "num_citations": 9, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=15779821433903919753", "cites_id": ["15779821433903919753"]}, "4fD1l28AAAAJ:IjCSPb-OGe4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "The HCCL-CUHK System for the Voice Conversion Challenge 2018.", "pub_year": "2018"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:IjCSPb-OGe4C", "num_citations": 9, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=16953365750147393440", "cites_id": ["16953365750147393440"]}, "4fD1l28AAAAJ:maZDTaKrznsC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Unifying one-shot voice conversion and cloning with disentangled speech representations", "pub_year": "2024"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:maZDTaKrznsC", "num_citations": 8, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=16879795714303340611", "cites_id": ["16879795714303340611"]}, "4fD1l28AAAAJ:TFP_iSt0sucC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "ASR-Robust Natural Language Understanding on ASR-GLUE dataset", "pub_year": "2022"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:TFP_iSt0sucC", "num_citations": 7, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=9791985070256651187", "cites_id": ["9791985070256651187"]}, "4fD1l28AAAAJ:35N4QoGY0k4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Longcat-flash-omni technical report", "pub_year": "2025"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:35N4QoGY0k4C", "num_citations": 6, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=7156815273256674036", "cites_id": ["7156815273256674036"]}, "4fD1l28AAAAJ:HDshCWvjkbEC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "SnakeGAN: A universal vocoder leveraging DDSP prior knowledge and periodic inductive bias", "pub_year": "2023"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:HDshCWvjkbEC", "num_citations": 5, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=6012164668208976167", "cites_id": ["6012164668208976167"]}, "4fD1l28AAAAJ:kNdYIx-mwKoC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Exploring cross-lingual singing voice synthesis using speech data", "pub_year": "2021"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:kNdYIx-mwKoC", "num_citations": 4, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=34018922997496457", "cites_id": ["34018922997496457"]}, "4fD1l28AAAAJ:ns9cj8rnVeAC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Hifi-codec: Group-residual vector quantization for high fidelity audio codec, 2023"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:ns9cj8rnVeAC", "num_citations": 4, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17010774209683464259", "cites_id": ["17010774209683464259"]}, "4fD1l28AAAAJ:BqipwSGYUEgC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "A system for Converting English text into speech", "pub_year": "2004"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:BqipwSGYUEgC", "num_citations": 3, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=598645242096613321", "cites_id": ["598645242096613321"]}, "4fD1l28AAAAJ:GnPB-g6toBAC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "HiFi-Codec: Group-residual Vector quantization for High Fidelity Audio Codec. abs/2305.02765 (2023)", "pub_year": "2023"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:GnPB-g6toBAC", "num_citations": 2, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=11251735287656071084", "cites_id": ["11251735287656071084"]}, "4fD1l28AAAAJ:zYLM7Y9cAGgC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Recurrent neural network language model training using natural gradient", "pub_year": "2019"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:zYLM7Y9cAGgC", "num_citations": 2, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=3386412063819448796", "cites_id": ["3386412063819448796"]}, "4fD1l28AAAAJ:NhqRSupF_l8C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "MMDuet2: Enhancing Proactive Interaction of Video MLLMs with Multi-Turn Reinforcement Learning", "pub_year": "2025"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:NhqRSupF_l8C", "num_citations": 1, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=11896996958759975319", "cites_id": ["11896996958759975319"]}, "4fD1l28AAAAJ:8AbLer7MMksC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "UniAudio 2.0: A Unified Audio Language Model with Text-Aligned Factorized Audio Tokenization", "pub_year": "2026"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:8AbLer7MMksC", "num_citations": 0}, "4fD1l28AAAAJ:08ZZubdj9fEC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "HeartMuLa: A Family of Open Sourced Music Foundation Models", "pub_year": "2026"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:08ZZubdj9fEC", "num_citations": 0}, "4fD1l28AAAAJ:xtRiw3GOFMkC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning", "pub_year": "2025"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:xtRiw3GOFMkC", "num_citations": 0}, "4fD1l28AAAAJ:R3hNpaxXUhUC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "SimpleSpeech 2: Towards Simple and Efficient Text-to-Speech with Flow-based Scalar Latent Transformer Diffusion Models", "pub_year": "2024"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:R3hNpaxXUhUC", "num_citations": 0}, "4fD1l28AAAAJ:bEWYMUwI8FkC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "ASR-Robust Spoken Language Understanding on ASR-GLUE dataset", "pub_year": "2022"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:bEWYMUwI8FkC", "num_citations": 0}, "4fD1l28AAAAJ:L8Ckcad2t8MC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "A Flexible Neural Encoder-Decoder Framework for Multi-Faceted Voice Transformation", "pub_year": "2021"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:L8Ckcad2t8MC", "num_citations": 0}, "4fD1l28AAAAJ:hMod-77fHWUC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "EXPLICIT PROSODY MODELLING AND DEEP SPEAKER EMBEDDING LEARNING FOR NON-STANDARD VOICE CONVERSION"}, "filled": false, "author_pub_id": "4fD1l28AAAAJ:hMod-77fHWUC", "num_citations": 0}}, "citedby5y": 2402, "hindex": 24, "hindex5y": 24, "i10index": 42, "i10index5y": 40, "cites_per_year": {"2019": 24, "2020": 97, "2021": 155, "2022": 237, "2023": 301, "2024": 557, "2025": 1015, "2026": 129}, "updated": "2026-02-20 08:49:30.270259"}